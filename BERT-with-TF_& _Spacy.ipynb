{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.147462,"end_time":"2021-10-23T12:27:11.408781","exception":false,"start_time":"2021-10-23T12:27:11.261319","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:53:26.230722Z","iopub.execute_input":"2021-11-14T14:53:26.231468Z","iopub.status.idle":"2021-11-14T14:53:26.254532Z","shell.execute_reply.started":"2021-11-14T14:53:26.231375Z","shell.execute_reply":"2021-11-14T14:53:26.253807Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### 1.1. Install Packages","metadata":{"papermill":{"duration":0.059106,"end_time":"2021-10-23T12:27:11.529726","exception":false,"start_time":"2021-10-23T12:27:11.47062","status":"completed"},"tags":[]}},{"cell_type":"code","source":"! python -m pip install tf-models-nightly --no-deps -q\n! python -m pip install tf-models-official==2.4.0 -q\n! python -m pip install tensorflow-gpu==2.4.1 -q\n! python -m pip install tensorflow-text==2.4.1 -q\n! python -m spacy download en_core_web_sm -q\n! python -m spacy validate ","metadata":{"papermill":{"duration":153.059377,"end_time":"2021-10-23T12:29:44.649056","exception":false,"start_time":"2021-10-23T12:27:11.589679","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:53:28.781280Z","iopub.execute_input":"2021-11-14T14:53:28.781696Z","iopub.status.idle":"2021-11-14T14:55:58.433857Z","shell.execute_reply.started":"2021-11-14T14:53:28.781654Z","shell.execute_reply":"2021-11-14T14:55:58.433051Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### 1.2. Import Libraries","metadata":{"papermill":{"duration":0.062903,"end_time":"2021-10-23T12:29:44.774993","exception":false,"start_time":"2021-10-23T12:29:44.71209","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Preprocessing\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport re\nimport string\nfrom bs4 import BeautifulSoup as bs\n# Model Training\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nfrom tensorflow.keras import layers, Model\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.metrics import BinaryAccuracy\nfrom tensorflow.keras.losses import BinaryCrossentropy\nimport official.nlp.optimization\nfrom official.nlp.optimization import create_optimizer # AdamW optimizer\nfrom sklearn.metrics import roc_curve, confusion_matrix\n# Visualization\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\n# Version\nfrom platform import python_version\n\nprint(f'TensorFlow Version: {tf.__version__}')\nprint(f'Python Version: {python_version()}')","metadata":{"papermill":{"duration":3.919414,"end_time":"2021-10-23T12:29:48.756362","exception":false,"start_time":"2021-10-23T12:29:44.836948","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:56:03.692363Z","iopub.execute_input":"2021-11-14T14:56:03.692954Z","iopub.status.idle":"2021-11-14T14:56:08.802427Z","shell.execute_reply.started":"2021-11-14T14:56:03.692914Z","shell.execute_reply":"2021-11-14T14:56:08.800834Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### 1.3. Configure Settings","metadata":{"papermill":{"duration":0.087101,"end_time":"2021-10-23T12:29:48.954699","exception":false,"start_time":"2021-10-23T12:29:48.867598","status":"completed"},"tags":[]}},{"cell_type":"code","source":"RANDOM_SEED = 123\nnlp = spacy.load('en_core_web_sm') \npd.set_option('display.max_colwidth', None) # Expand DataFrame column width\nrcParams['figure.figsize'] = (10, 6) # Custom plot dimensions\nsns.set_theme(palette='muted', style='whitegrid') # Seaborn plot theme","metadata":{"papermill":{"duration":1.250732,"end_time":"2021-10-23T12:29:50.2683","exception":false,"start_time":"2021-10-23T12:29:49.017568","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:56:12.138355Z","iopub.execute_input":"2021-11-14T14:56:12.139017Z","iopub.status.idle":"2021-11-14T14:56:12.829524Z","shell.execute_reply.started":"2021-11-14T14:56:12.138977Z","shell.execute_reply":"2021-11-14T14:56:12.828780Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### 1.4. Load the Training and Test Files","metadata":{"papermill":{"duration":0.062644,"end_time":"2021-10-23T12:29:50.39735","exception":false,"start_time":"2021-10-23T12:29:50.334706","status":"completed"},"tags":[]}},{"cell_type":"code","source":"path = '../input/nlp-getting-started/train.csv'\ndf = pd.read_csv(path)\nprint(df.shape)\ndf.head()","metadata":{"papermill":{"duration":0.134389,"end_time":"2021-10-23T12:29:50.593755","exception":false,"start_time":"2021-10-23T12:29:50.459366","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:56:17.493947Z","iopub.execute_input":"2021-11-14T14:56:17.494636Z","iopub.status.idle":"2021-11-14T14:56:17.550884Z","shell.execute_reply.started":"2021-11-14T14:56:17.494598Z","shell.execute_reply":"2021-11-14T14:56:17.550135Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"path_test = '../input/nlp-getting-started/test.csv'\ndf_test = pd.read_csv(path_test)\nprint(df_test.shape)\ndf_test.head()","metadata":{"papermill":{"duration":0.159885,"end_time":"2021-10-23T12:29:50.866055","exception":false,"start_time":"2021-10-23T12:29:50.70617","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:56:19.366863Z","iopub.execute_input":"2021-11-14T14:56:19.367394Z","iopub.status.idle":"2021-11-14T14:56:19.408249Z","shell.execute_reply.started":"2021-11-14T14:56:19.367351Z","shell.execute_reply":"2021-11-14T14:56:19.407611Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# 2. Raw Data Analysis","metadata":{"papermill":{"duration":0.118222,"end_time":"2021-10-23T12:29:51.110677","exception":false,"start_time":"2021-10-23T12:29:50.992455","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df.info(verbose=False)","metadata":{"papermill":{"duration":0.082839,"end_time":"2021-10-23T12:29:51.280174","exception":false,"start_time":"2021-10-23T12:29:51.197335","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:56:21.412143Z","iopub.execute_input":"2021-11-14T14:56:21.412701Z","iopub.status.idle":"2021-11-14T14:56:21.435232Z","shell.execute_reply.started":"2021-11-14T14:56:21.412656Z","shell.execute_reply":"2021-11-14T14:56:21.434300Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df_test.info(verbose=False)","metadata":{"papermill":{"duration":0.088896,"end_time":"2021-10-23T12:29:51.45156","exception":false,"start_time":"2021-10-23T12:29:51.362664","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:56:22.562695Z","iopub.execute_input":"2021-11-14T14:56:22.563556Z","iopub.status.idle":"2021-11-14T14:56:22.573224Z","shell.execute_reply.started":"2021-11-14T14:56:22.563508Z","shell.execute_reply":"2021-11-14T14:56:22.572423Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df['text'].describe()","metadata":{"papermill":{"duration":0.100543,"end_time":"2021-10-23T12:29:51.637181","exception":false,"start_time":"2021-10-23T12:29:51.536638","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:56:23.682437Z","iopub.execute_input":"2021-11-14T14:56:23.682985Z","iopub.status.idle":"2021-11-14T14:56:23.695598Z","shell.execute_reply.started":"2021-11-14T14:56:23.682945Z","shell.execute_reply":"2021-11-14T14:56:23.694893Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df_test['text'].describe()","metadata":{"papermill":{"duration":0.081865,"end_time":"2021-10-23T12:29:51.786424","exception":false,"start_time":"2021-10-23T12:29:51.704559","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:56:24.787292Z","iopub.execute_input":"2021-11-14T14:56:24.787838Z","iopub.status.idle":"2021-11-14T14:56:24.799759Z","shell.execute_reply.started":"2021-11-14T14:56:24.787800Z","shell.execute_reply":"2021-11-14T14:56:24.798715Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### 2.1. Remove Duplicate Data\n","metadata":{"papermill":{"duration":0.06496,"end_time":"2021-10-23T12:29:51.916575","exception":false,"start_time":"2021-10-23T12:29:51.851615","status":"completed"},"tags":[]}},{"cell_type":"code","source":"duplicates = df[df.duplicated(['text', 'target'], keep=False)]\nprint(f'Train Duplicate Entries (text, target): {len(duplicates)}')\nduplicates.head()","metadata":{"papermill":{"duration":0.09018,"end_time":"2021-10-23T12:29:52.072315","exception":false,"start_time":"2021-10-23T12:29:51.982135","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:56:29.057763Z","iopub.execute_input":"2021-11-14T14:56:29.058036Z","iopub.status.idle":"2021-11-14T14:56:29.076547Z","shell.execute_reply.started":"2021-11-14T14:56:29.058008Z","shell.execute_reply":"2021-11-14T14:56:29.075861Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df.drop_duplicates(['text', 'target'], inplace=True, ignore_index=True)\nprint(df.shape, df_test.shape)","metadata":{"papermill":{"duration":0.12414,"end_time":"2021-10-23T12:29:52.301973","exception":false,"start_time":"2021-10-23T12:29:52.177833","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:56:32.477235Z","iopub.execute_input":"2021-11-14T14:56:32.477688Z","iopub.status.idle":"2021-11-14T14:56:32.488510Z","shell.execute_reply.started":"2021-11-14T14:56:32.477646Z","shell.execute_reply":"2021-11-14T14:56:32.487613Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Duplicates with the same keyword and text share both target classes. The count is low enough to manually review these tweets and drop those with the incorrect target label. ","metadata":{"papermill":{"duration":0.105607,"end_time":"2021-10-23T12:29:52.526242","exception":false,"start_time":"2021-10-23T12:29:52.420635","status":"completed"},"tags":[]}},{"cell_type":"code","source":"new_duplicates = df[df.duplicated(['keyword', 'text'], keep=False)]\n\nprint(f'Train Duplicate Entries (keyword, text): {len(new_duplicates)}')\nnew_duplicates[['text', 'target']].sort_values(by='text')","metadata":{"papermill":{"duration":0.153488,"end_time":"2021-10-23T12:29:52.784646","exception":false,"start_time":"2021-10-23T12:29:52.631158","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:56:34.297014Z","iopub.execute_input":"2021-11-14T14:56:34.297293Z","iopub.status.idle":"2021-11-14T14:56:34.318707Z","shell.execute_reply.started":"2021-11-14T14:56:34.297263Z","shell.execute_reply":"2021-11-14T14:56:34.318001Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Drop the target label that is false for each duplicate pair\ndf.drop([4253, 4193, 2802, 4554, 4182, 3212, 4249, 4259, 6535, 4319, 4239, 606, 3936, 6018, 5573], inplace=True)","metadata":{"papermill":{"duration":0.121655,"end_time":"2021-10-23T12:29:53.017558","exception":false,"start_time":"2021-10-23T12:29:52.895903","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:56:38.358180Z","iopub.execute_input":"2021-11-14T14:56:38.358531Z","iopub.status.idle":"2021-11-14T14:56:38.364804Z","shell.execute_reply.started":"2021-11-14T14:56:38.358496Z","shell.execute_reply":"2021-11-14T14:56:38.364144Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Reset the dataframe index to account for missing numbers\ndf = df.reset_index(drop=True)\ndf","metadata":{"papermill":{"duration":0.136121,"end_time":"2021-10-23T12:29:53.263792","exception":false,"start_time":"2021-10-23T12:29:53.127671","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:56:40.447132Z","iopub.execute_input":"2021-11-14T14:56:40.447790Z","iopub.status.idle":"2021-11-14T14:56:40.462735Z","shell.execute_reply.started":"2021-11-14T14:56:40.447757Z","shell.execute_reply":"2021-11-14T14:56:40.462087Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### 2.2. Examine the Target Data Balance","metadata":{"papermill":{"duration":0.067183,"end_time":"2021-10-23T12:29:53.402632","exception":false,"start_time":"2021-10-23T12:29:53.335449","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df['target'].value_counts() / len(df)","metadata":{"papermill":{"duration":0.077767,"end_time":"2021-10-23T12:29:53.547882","exception":false,"start_time":"2021-10-23T12:29:53.470115","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:56:44.007045Z","iopub.execute_input":"2021-11-14T14:56:44.007625Z","iopub.status.idle":"2021-11-14T14:56:44.015446Z","shell.execute_reply.started":"2021-11-14T14:56:44.007587Z","shell.execute_reply":"2021-11-14T14:56:44.014722Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### 2.3. Check for Null Values","metadata":{"papermill":{"duration":0.067554,"end_time":"2021-10-23T12:29:53.683992","exception":false,"start_time":"2021-10-23T12:29:53.616438","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def null_table(data): \n    # Bool is True if values are not null\n    null_list = []\n\n    for i in data:\n        if data[i].notnull().any():\n            null_list.append(data[i].notnull().value_counts())\n    \n    return pd.DataFrame(pd.concat(null_list, axis=1).T)","metadata":{"papermill":{"duration":0.076547,"end_time":"2021-10-23T12:29:53.828483","exception":false,"start_time":"2021-10-23T12:29:53.751936","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:56:45.791861Z","iopub.execute_input":"2021-11-14T14:56:45.792490Z","iopub.status.idle":"2021-11-14T14:56:45.797302Z","shell.execute_reply.started":"2021-11-14T14:56:45.792451Z","shell.execute_reply":"2021-11-14T14:56:45.796555Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"null_table(df)","metadata":{"papermill":{"duration":0.091269,"end_time":"2021-10-23T12:29:53.987964","exception":false,"start_time":"2021-10-23T12:29:53.896695","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:56:47.593270Z","iopub.execute_input":"2021-11-14T14:56:47.594128Z","iopub.status.idle":"2021-11-14T14:56:47.619011Z","shell.execute_reply.started":"2021-11-14T14:56:47.594067Z","shell.execute_reply":"2021-11-14T14:56:47.618061Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"null_table(df_test)","metadata":{"papermill":{"duration":0.088131,"end_time":"2021-10-23T12:29:54.148157","exception":false,"start_time":"2021-10-23T12:29:54.060026","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:56:48.957152Z","iopub.execute_input":"2021-11-14T14:56:48.957817Z","iopub.status.idle":"2021-11-14T14:56:48.976663Z","shell.execute_reply.started":"2021-11-14T14:56:48.957779Z","shell.execute_reply":"2021-11-14T14:56:48.975979Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"A check for null values in the data shows that the `location` column is missing a significant amount information. This could interfere with model performance, so that data will be excluded. In contrast, the `keyword` column has a more acceptable count of missing values so we can fill these with extracted keywords using spaCy.","metadata":{"papermill":{"duration":0.069521,"end_time":"2021-10-23T12:29:54.289955","exception":false,"start_time":"2021-10-23T12:29:54.220434","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# 3. Text Preprocessing and EDA","metadata":{"papermill":{"duration":0.068377,"end_time":"2021-10-23T12:29:54.426769","exception":false,"start_time":"2021-10-23T12:29:54.358392","status":"completed"},"tags":[]}},{"cell_type":"code","source":"text = df['text']\ntarget = df['target']\n\ntest_text = df_test['text']\n\n# Print random samples from the training text \nfor i in np.random.randint(500, size=5):\n    print(f'Tweet #{i}: ', text[i], '=> Target: ', target[i], end='\\n' * 2)","metadata":{"papermill":{"duration":0.080225,"end_time":"2021-10-23T12:29:54.574956","exception":false,"start_time":"2021-10-23T12:29:54.494731","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:56:52.292486Z","iopub.execute_input":"2021-11-14T14:56:52.292890Z","iopub.status.idle":"2021-11-14T14:56:52.307484Z","shell.execute_reply.started":"2021-11-14T14:56:52.292855Z","shell.execute_reply":"2021-11-14T14:56:52.306844Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"The next task is to build a text standardization function that is specific to the content found in tweets. Aside from punctuation, there are urls, abbreviations, entities, retweets, digits, stopwords, and of course emojis. The words in each tweet will also be lemmatized or reduced to their root form using the spaCy library. We'll start by building a lookup dictionary with common twitter phrase abbreviations. Tweet terms that match keys in the lookup dictionary will be expanded to their non-abbreviated form.","metadata":{"papermill":{"duration":0.068947,"end_time":"2021-10-23T12:29:54.713524","exception":false,"start_time":"2021-10-23T12:29:54.644577","status":"completed"},"tags":[]}},{"cell_type":"code","source":"lookup_dict = {\n  'abt' : 'about',\n  'afaik' : 'as far as i know',\n  'bc' : 'because',\n  'bfn' : 'bye for now',\n  'bgd' : 'background',\n  'bh' : 'blockhead',\n  'br' : 'best regards',\n  'btw' : 'by the way',\n  'cc': 'carbon copy',\n  'chk' : 'check',\n  'dam' : 'do not annoy me',\n  'dd' : 'dear daughter',\n  'df': 'dear fiance',\n  'ds' : 'dear son',\n  'dyk' : 'did you know',\n  'em': 'email',\n  'ema' : 'email address',\n  'ftf' : 'face to face',\n  'fb' : 'facebook',\n  'ff' : 'follow friday', \n  'fotd' : 'find of the day',\n  'ftw': 'for the win',\n  'fwiw' : 'for what it is worth',\n  'gts' : 'guess the song',\n  'hagn' : 'have a good night',\n  'hand' : 'have a nice day',\n  'hotd' : 'headline of the day',\n  'ht' : 'heard through',\n  'hth' : 'hope that helps',\n  'ic' : 'i see',\n  'icymi' : 'in case you missed it',\n  'idk' : 'i do not know',\n  'ig': 'instagram',\n  'iirc' : 'if i remember correctly',\n  'imho' : 'in my humble opinion',\n  'imo' : 'in my opinion',\n  'irl' : 'in real life',\n  'iwsn' : 'i want sex now',\n  'jk' : 'just kidding',\n  'jsyk' : 'just so you know',\n  'jv' : 'joint venture',\n  'kk' : 'cool cool',\n  'kyso' : 'knock your socks off',\n  'lmao' : 'laugh my ass off',\n  'lmk' : 'let me know', \n  'lo' : 'little one',\n  'lol' : 'laugh out loud',\n  'mm' : 'music monday',\n  'mirl' : 'meet in real life',\n  'mrjn' : 'marijuana',\n  'nbd' : 'no big deal',\n  'nct' : 'nobody cares though',\n  'njoy' : 'enjoy',\n  'nsfw' : 'not safe for work',\n  'nts' : 'note to self',\n  'oh' : 'overheard',\n  'omg': 'oh my god',\n  'oomf' : 'one of my friends',\n  'orly' : 'oh really',\n  'plmk' : 'please let me know',\n  'pnp' : 'party and play', \n  'qotd' : 'quote of the day',\n  're' : 'in reply to in regards to',\n  'rtq' : 'read the question',\n  'rt' : 'retweet',\n  'sfw' : 'safe for work',\n  'smdh' : 'shaking my damn head', \n  'smh' : 'shaking my head',\n  'so' : 'significant other',\n  'srs' : 'serious',\n  'tftf' : 'thanks for the follow',\n  'tftt' : 'thanks for this tweet',\n  'tj' : 'tweetjack',\n  'tl' : 'timeline',\n  'tldr' : 'too long did not read',\n  'tmb' : 'tweet me back',\n  'tt' : 'trending topic',\n  'ty' : 'thank you',\n  'tyia' : 'thank you in advance',\n  'tyt' : 'take your time',\n  'tyvw' : 'thank you very much',\n  'w': 'with', \n  'wtv' : 'whatever',\n  'ygtr' : 'you got that right',\n  'ykwim' : 'you know what i mean',\n  'ykyat' : 'you know you are addicted to',\n  'ymmv' : 'your mileage may vary',\n  'yolo' : 'you only live once',\n  'yoyo' : 'you are on your own',\n  'yt': 'youtube',\n  'yw' : 'you are welcome',\n  'zomg' : 'oh my god to the maximum'\n}","metadata":{"papermill":{"duration":0.084387,"end_time":"2021-10-23T12:29:54.867078","exception":false,"start_time":"2021-10-23T12:29:54.782691","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:56:55.598193Z","iopub.execute_input":"2021-11-14T14:56:55.598579Z","iopub.status.idle":"2021-11-14T14:56:55.611242Z","shell.execute_reply.started":"2021-11-14T14:56:55.598546Z","shell.execute_reply":"2021-11-14T14:56:55.610520Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### 3.1. Text Standardization Functions","metadata":{"papermill":{"duration":0.068176,"end_time":"2021-10-23T12:29:55.003733","exception":false,"start_time":"2021-10-23T12:29:54.935557","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def lemmatize_text(text, nlp=nlp):\n    doc = nlp(text)    \n    lemma_sent = [i.lemma_ for i in doc if not i.is_stop]    \n    \n    return ' '.join(lemma_sent)  \n\ndef abbrev_conversion(text):\n    words = text.split() \n    abbrevs_removed = [] \n    \n    for i in words:\n        if i in lookup_dict:\n            i = lookup_dict[i]\n        abbrevs_removed.append(i)\n            \n    return ' '.join(abbrevs_removed)\n\ndef standardize_text(text_data):    \n    entity_pattern = re.compile(r'(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)') \n    url_pattern = re.compile(r'(?:\\@|http?\\://|https?\\://|www)\\S+')\n    retweet_pattern = re.compile(r'^(RT|RT:)\\s+')\n    digit_pattern = re.compile(r'[\\d]+')\n    # From https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               u\"\\U0001f926-\\U0001f937\"\n                               u\"\\U00010000-\\U0010ffff\"\n                               u\"\\u2640-\\u2642\"\n                               u\"\\u2600-\\u2B55\"\n                               u\"\\u200d\"\n                               u\"\\u23cf\"\n                               u\"\\u23e9\"\n                               u\"\\u231a\"\n                               u\"\\ufe0f\"  # dingbats\n                               u\"\\u3030\"\n                               \"]+\", flags=re.UNICODE)\n    \n    # Remove urls\n    url_strip = text_data.apply(lambda x: re.sub(url_pattern, '', x) if pd.isna(x) != True else x)\n    # Parse the HTML\n    html_parse = url_strip.apply(lambda x: bs(x, 'html.parser').get_text() if pd.isna(x) != True else x)\n    # Remove rewteets\n    retweet_strip = html_parse.apply(lambda x: re.sub(retweet_pattern, '', x) if pd.isna(x) != True else x)\n    # Remove emojis\n    emoji_strip = retweet_strip.apply(lambda x: re.sub(emoji_pattern, '', x) if pd.isna(x) != True else x)\n    # Remove entities\n    entity_strip = emoji_strip.apply(lambda x: re.sub(entity_pattern, '', x) if pd.isna(x) != True else x)\n    # Lowercase the strings\n    lowercase = entity_strip.apply(lambda x: str.lower(x) if pd.isna(x) != True else x)               \n    # Remove punctuation\n    punct_strip = lowercase.apply(lambda x: re.sub(f'[{re.escape(string.punctuation)}]', '', x) if pd.isna(x) != True else x) \n    # Convert abbreviations to words\n    abbrev_converted = punct_strip.apply(lambda x: abbrev_conversion(x) if pd.isna(x) != True else x)\n    # Remove digits\n    digit_strip = abbrev_converted.apply(lambda x: re.sub(digit_pattern, '', x) if pd.isna(x) != True else x)    \n    # Lemmatize text and filter stopwords\n    lemma_and_stop = digit_strip.apply(lambda x: lemmatize_text(x) if pd.isna(x) != True else x)\n    \n    return lemma_and_stop","metadata":{"papermill":{"duration":0.085547,"end_time":"2021-10-23T12:29:55.162747","exception":false,"start_time":"2021-10-23T12:29:55.0772","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:56:59.483955Z","iopub.execute_input":"2021-11-14T14:56:59.484662Z","iopub.status.idle":"2021-11-14T14:56:59.500103Z","shell.execute_reply.started":"2021-11-14T14:56:59.484624Z","shell.execute_reply":"2021-11-14T14:56:59.499308Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"clean_text = np.asarray(standardize_text(text))\ntest_clean_text = np.asarray(standardize_text(test_text))\n\n# Print random samples from the cleaned training text\nfor i in np.random.randint(500, size=5):\n    print(f'Tweet #{i}: ', clean_text[i], '=> Target: ', target[i], end='\\n' * 2)","metadata":{"papermill":{"duration":76.214584,"end_time":"2021-10-23T12:31:11.445619","exception":false,"start_time":"2021-10-23T12:29:55.231035","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:57:01.672251Z","iopub.execute_input":"2021-11-14T14:57:01.672926Z","iopub.status.idle":"2021-11-14T14:58:07.304169Z","shell.execute_reply.started":"2021-11-14T14:57:01.672889Z","shell.execute_reply":"2021-11-14T14:58:07.303383Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"The text is now in a cleaner format for training, but before taking the next step we should explore it with some visualizations.","metadata":{"papermill":{"duration":0.069217,"end_time":"2021-10-23T12:31:11.621921","exception":false,"start_time":"2021-10-23T12:31:11.552704","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df['clean_text'] = pd.DataFrame(clean_text)\ndf_test['clean_text'] = pd.DataFrame(test_clean_text)","metadata":{"papermill":{"duration":0.076629,"end_time":"2021-10-23T12:31:11.767155","exception":false,"start_time":"2021-10-23T12:31:11.690526","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:58:10.991947Z","iopub.execute_input":"2021-11-14T14:58:10.992639Z","iopub.status.idle":"2021-11-14T14:58:10.998188Z","shell.execute_reply.started":"2021-11-14T14:58:10.992600Z","shell.execute_reply":"2021-11-14T14:58:10.997117Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"### 3.2. Plot Tweet Length Histogram","metadata":{"papermill":{"duration":0.068302,"end_time":"2021-10-23T12:31:11.904709","exception":false,"start_time":"2021-10-23T12:31:11.836407","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df['tweet_len'] = df['clean_text'].apply(lambda x: len(x))\n\ncount, bin_edges = np.histogram(df['tweet_len'])\nsns.histplot(data=df, x=df['tweet_len'], bins=bin_edges, hue=df['target'])\nplt.title('Tweet Length Frequency')\nplt.xlabel('Length of Tweets')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"papermill":{"duration":0.363648,"end_time":"2021-10-23T12:31:12.337085","exception":false,"start_time":"2021-10-23T12:31:11.973437","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:58:14.327237Z","iopub.execute_input":"2021-11-14T14:58:14.327763Z","iopub.status.idle":"2021-11-14T14:58:14.700624Z","shell.execute_reply.started":"2021-11-14T14:58:14.327725Z","shell.execute_reply":"2021-11-14T14:58:14.699932Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"A tweet length histogram shows that longer tweets at the end of the distribution have a greater frequency of disaster. ","metadata":{"papermill":{"duration":0.07107,"end_time":"2021-10-23T12:31:12.492223","exception":false,"start_time":"2021-10-23T12:31:12.421153","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### 3.3. Display Non-Disaster and Disaster WordClouds","metadata":{"papermill":{"duration":0.070109,"end_time":"2021-10-23T12:31:12.631903","exception":false,"start_time":"2021-10-23T12:31:12.561794","status":"completed"},"tags":[]}},{"cell_type":"code","source":"word_cloud_0 = WordCloud(collocations=False, background_color='white').generate(' '.join(df['clean_text'][df['target']==0]))\nplt.imshow(word_cloud_0, interpolation='bilinear')\nplt.title('Non-Disaster Wordcloud (0)')\nplt.axis('off')\nplt.show()","metadata":{"papermill":{"duration":0.565623,"end_time":"2021-10-23T12:31:13.267295","exception":false,"start_time":"2021-10-23T12:31:12.701672","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:58:18.067208Z","iopub.execute_input":"2021-11-14T14:58:18.067854Z","iopub.status.idle":"2021-11-14T14:58:18.828798Z","shell.execute_reply.started":"2021-11-14T14:58:18.067814Z","shell.execute_reply":"2021-11-14T14:58:18.828128Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"word_cloud_1 = WordCloud(collocations=False, background_color='black').generate(' '.join(df['clean_text'][df['target']==1]))\nplt.imshow(word_cloud_1, interpolation='bilinear')\nplt.title('Disaster Wordcloud (1)')\nplt.axis('off')\nplt.show()","metadata":{"papermill":{"duration":0.546877,"end_time":"2021-10-23T12:31:13.891239","exception":false,"start_time":"2021-10-23T12:31:13.344362","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:58:21.217118Z","iopub.execute_input":"2021-11-14T14:58:21.217388Z","iopub.status.idle":"2021-11-14T14:58:22.166542Z","shell.execute_reply.started":"2021-11-14T14:58:21.217361Z","shell.execute_reply":"2021-11-14T14:58:22.165897Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"### 3.4. Remove the Word *new* from Text","metadata":{"papermill":{"duration":0.0835,"end_time":"2021-10-23T12:31:14.056675","exception":false,"start_time":"2021-10-23T12:31:13.973175","status":"completed"},"tags":[]}},{"cell_type":"code","source":"pattern_new = re.compile(r'\\bnew\\b')\n\nprint('Training Counts of \\'new\\': ', len(re.findall(pattern_new, ' '.join(df['clean_text']))))\nprint('Test Counts of \\'new\\': ', len(re.findall(pattern_new, ' '.join(df_test['clean_text']))))","metadata":{"papermill":{"duration":0.108002,"end_time":"2021-10-23T12:31:14.245984","exception":false,"start_time":"2021-10-23T12:31:14.137982","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:58:26.597952Z","iopub.execute_input":"2021-11-14T14:58:26.598670Z","iopub.status.idle":"2021-11-14T14:58:26.626616Z","shell.execute_reply.started":"2021-11-14T14:58:26.598633Z","shell.execute_reply":"2021-11-14T14:58:26.625743Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Clean the word 'new' from the training and test data\ndf['clean_text'] = df['clean_text'].apply(lambda x: re.sub(pattern_new, '', x) if pd.isna(x) != True else x)\ndf_test['clean_text'] = df_test['clean_text'].apply(lambda x: re.sub(pattern_new, '', x) if pd.isna(x) != True else x)","metadata":{"papermill":{"duration":0.135099,"end_time":"2021-10-23T12:31:14.461981","exception":false,"start_time":"2021-10-23T12:31:14.326882","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:58:31.296931Z","iopub.execute_input":"2021-11-14T14:58:31.297683Z","iopub.status.idle":"2021-11-14T14:58:31.355539Z","shell.execute_reply.started":"2021-11-14T14:58:31.297642Z","shell.execute_reply":"2021-11-14T14:58:31.354768Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"print('Training Counts of \\'new\\': ', len(re.findall(pattern_new, ' '.join(df['clean_text']))))\nprint('Test Counts of \\'new\\': ', len(re.findall(pattern_new, ' '.join(df_test['clean_text']))))","metadata":{"papermill":{"duration":0.106994,"end_time":"2021-10-23T12:31:14.650296","exception":false,"start_time":"2021-10-23T12:31:14.543302","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:58:32.371928Z","iopub.execute_input":"2021-11-14T14:58:32.372474Z","iopub.status.idle":"2021-11-14T14:58:32.397134Z","shell.execute_reply.started":"2021-11-14T14:58:32.372432Z","shell.execute_reply":"2021-11-14T14:58:32.396434Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"### 3.5. Fill Missing Keywords\n\nThere are fifty-six missing keywords in the training data set and twenty-six in the test set. We will use spaCy's part of speech tagging feature to fill the missing words. The process is as follows:\n1. Create a list of potential keywords by tagging nouns, pronouns, and adjectives\n2. Use a sentence encoder to embed the list of potential keywords and the comparison text\n3. Calculate vector distances using the cosine similarity function\n4. Sort the vectors and select the top keyword for each tweet\n\nThe sentence encoder used here is loaded from TensorFlow Hub. It is a pre-trained universal sentence encoder published by Google for use in natural language tasks.","metadata":{"papermill":{"duration":0.081615,"end_time":"2021-10-23T12:31:14.812795","exception":false,"start_time":"2021-10-23T12:31:14.73118","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Load the sentence encoder\nsentence_enc = hub.load('https://tfhub.dev/google/universal-sentence-encoder/4')","metadata":{"papermill":{"duration":22.849206,"end_time":"2021-10-23T12:31:37.744564","exception":false,"start_time":"2021-10-23T12:31:14.895358","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:58:36.108041Z","iopub.execute_input":"2021-11-14T14:58:36.108872Z","iopub.status.idle":"2021-11-14T14:59:01.498941Z","shell.execute_reply.started":"2021-11-14T14:58:36.108834Z","shell.execute_reply":"2021-11-14T14:59:01.497363Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"### 3.6. Keyword Extract and Fill Functions","metadata":{"papermill":{"duration":0.085935,"end_time":"2021-10-23T12:31:37.91465","exception":false,"start_time":"2021-10-23T12:31:37.828715","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def extract_keywords(text, nlp=nlp):\n    potential_keywords = []\n    TOP_KEYWORD = -1\n    # Create a list for keyword parts of speech\n    pos_tag = ['ADJ', 'NOUN', 'PROPN']\n    doc = nlp(text)\n    \n    for i in doc:\n        if i.pos_ in pos_tag:\n            potential_keywords.append(i.text)\n\n    document_embed = sentence_enc([text])\n    potential_embed = sentence_enc(potential_keywords)    \n    \n    vector_distances = cosine_similarity(document_embed, potential_embed)\n    keyword = [potential_keywords[i] for i in vector_distances.argsort()[0][TOP_KEYWORD:]]\n\n    return keyword\n\ndef keyword_filler(keyword, text):\n    if pd.isnull(keyword):\n        try:\n            keyword = extract_keywords(text)[0]\n        except:\n            keyword = '' \n        \n    return keyword","metadata":{"papermill":{"duration":0.094613,"end_time":"2021-10-23T12:31:38.095018","exception":false,"start_time":"2021-10-23T12:31:38.000405","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:59:03.917615Z","iopub.execute_input":"2021-11-14T14:59:03.918035Z","iopub.status.idle":"2021-11-14T14:59:03.932201Z","shell.execute_reply.started":"2021-11-14T14:59:03.917993Z","shell.execute_reply":"2021-11-14T14:59:03.931163Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"df['keyword_fill'] = pd.DataFrame(list(map(keyword_filler, df['keyword'], df['clean_text']))).astype(str)\ndf_test['keyword_fill'] = pd.DataFrame(list(map(keyword_filler, df_test['keyword'], df_test['clean_text']))).astype(str)\n\nprint('Null Training Keywords => ', df['keyword_fill'].isnull().any())\nprint('Null Test Keywords => ', df_test['keyword_fill'].isnull().any())","metadata":{"papermill":{"duration":2.702018,"end_time":"2021-10-23T12:31:40.882942","exception":false,"start_time":"2021-10-23T12:31:38.180924","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:59:05.461997Z","iopub.execute_input":"2021-11-14T14:59:05.462563Z","iopub.status.idle":"2021-11-14T14:59:07.965556Z","shell.execute_reply.started":"2021-11-14T14:59:05.462514Z","shell.execute_reply":"2021-11-14T14:59:07.964757Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"Now that the missing keywords are filled we should standardize them to ensure our keywords are clean and ready for training.","metadata":{"papermill":{"duration":0.084255,"end_time":"2021-10-23T12:31:41.052563","exception":false,"start_time":"2021-10-23T12:31:40.968308","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df['keyword_fill'] = pd.DataFrame(standardize_text(df['keyword_fill']))\ndf_test['keyword_fill'] = pd.DataFrame(standardize_text(df_test['keyword_fill']))","metadata":{"papermill":{"duration":57.112812,"end_time":"2021-10-23T12:32:38.248989","exception":false,"start_time":"2021-10-23T12:31:41.136177","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T14:59:10.136849Z","iopub.execute_input":"2021-11-14T14:59:10.137121Z","iopub.status.idle":"2021-11-14T14:59:53.751481Z","shell.execute_reply.started":"2021-11-14T14:59:10.137072Z","shell.execute_reply":"2021-11-14T14:59:53.750758Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"papermill":{"duration":0.16105,"end_time":"2021-10-23T12:32:38.584713","exception":false,"start_time":"2021-10-23T12:32:38.423663","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T15:00:10.918337Z","iopub.execute_input":"2021-11-14T15:00:10.918934Z","iopub.status.idle":"2021-11-14T15:00:10.948503Z","shell.execute_reply.started":"2021-11-14T15:00:10.918882Z","shell.execute_reply":"2021-11-14T15:00:10.947645Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"papermill":{"duration":0.098134,"end_time":"2021-10-23T12:32:38.769796","exception":false,"start_time":"2021-10-23T12:32:38.671662","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T15:00:12.372171Z","iopub.execute_input":"2021-11-14T15:00:12.372902Z","iopub.status.idle":"2021-11-14T15:00:12.388076Z","shell.execute_reply.started":"2021-11-14T15:00:12.372852Z","shell.execute_reply":"2021-11-14T15:00:12.387132Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"### 3.7. Plot the Keyword Frequencies ","metadata":{"papermill":{"duration":0.083744,"end_time":"2021-10-23T12:32:38.937307","exception":false,"start_time":"2021-10-23T12:32:38.853563","status":"completed"},"tags":[]}},{"cell_type":"code","source":"keyword_count_0 = pd.DataFrame(df['keyword_fill'][df['target']==0].value_counts().reset_index())\nkeyword_count_1 = pd.DataFrame(df['keyword_fill'][df['target']==1].value_counts().reset_index())","metadata":{"papermill":{"duration":0.098544,"end_time":"2021-10-23T12:32:39.119393","exception":false,"start_time":"2021-10-23T12:32:39.020849","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T15:00:14.522478Z","iopub.execute_input":"2021-11-14T15:00:14.523040Z","iopub.status.idle":"2021-11-14T15:00:14.537653Z","shell.execute_reply.started":"2021-11-14T15:00:14.523001Z","shell.execute_reply":"2021-11-14T15:00:14.536906Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"sns.barplot(data=keyword_count_0[:10], x='keyword_fill', y='index')\nplt.title('Non-Disaster Keyword Frequency (0)')\nplt.xlabel('Frequency')\nplt.ylabel('Top 10 Keywords')\nplt.show()","metadata":{"papermill":{"duration":0.310201,"end_time":"2021-10-23T12:32:39.513371","exception":false,"start_time":"2021-10-23T12:32:39.20317","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T15:00:16.417097Z","iopub.execute_input":"2021-11-14T15:00:16.417795Z","iopub.status.idle":"2021-11-14T15:00:16.696603Z","shell.execute_reply.started":"2021-11-14T15:00:16.417759Z","shell.execute_reply":"2021-11-14T15:00:16.695946Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"sns.barplot(data=keyword_count_1[:10], x='keyword_fill', y='index')\nplt.title('Disaster Keyword Frequency (1)')\nplt.xlabel('Frequency')\nplt.ylabel('Top 10 Keywords')\nplt.show()","metadata":{"papermill":{"duration":0.306676,"end_time":"2021-10-23T12:32:39.904912","exception":false,"start_time":"2021-10-23T12:32:39.598236","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T15:00:19.752012Z","iopub.execute_input":"2021-11-14T15:00:19.752317Z","iopub.status.idle":"2021-11-14T15:00:20.041639Z","shell.execute_reply.started":"2021-11-14T15:00:19.752284Z","shell.execute_reply":"2021-11-14T15:00:20.040933Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"A look at the top keywords for each class demonstrates the importance of observing the tweet itself to find context. At a glance, these keywords could belong to either class. That being the case, keywords can still emphasize important parts of speech that may be overlooked by the model. ","metadata":{"papermill":{"duration":0.084722,"end_time":"2021-10-23T12:32:40.075268","exception":false,"start_time":"2021-10-23T12:32:39.990546","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# 4. Prepare Data for Training","metadata":{"papermill":{"duration":0.084538,"end_time":"2021-10-23T12:32:40.245417","exception":false,"start_time":"2021-10-23T12:32:40.160879","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### 4.1. Select the Clean Text and Filled Keyword Columns","metadata":{"papermill":{"duration":0.08453,"end_time":"2021-10-23T12:32:40.5843","exception":false,"start_time":"2021-10-23T12:32:40.49977","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_features = df[['clean_text','keyword_fill']]\ntest_features = df_test[['clean_text', 'keyword_fill']]","metadata":{"papermill":{"duration":0.096994,"end_time":"2021-10-23T12:32:40.766536","exception":false,"start_time":"2021-10-23T12:32:40.669542","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T15:00:24.612257Z","iopub.execute_input":"2021-11-14T15:00:24.612824Z","iopub.status.idle":"2021-11-14T15:00:24.622847Z","shell.execute_reply.started":"2021-11-14T15:00:24.612787Z","shell.execute_reply":"2021-11-14T15:00:24.622105Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"train_features[:5]","metadata":{"papermill":{"duration":0.098983,"end_time":"2021-10-23T12:32:40.951773","exception":false,"start_time":"2021-10-23T12:32:40.85279","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T15:00:25.891919Z","iopub.execute_input":"2021-11-14T15:00:25.892513Z","iopub.status.idle":"2021-11-14T15:00:25.902489Z","shell.execute_reply.started":"2021-11-14T15:00:25.892471Z","shell.execute_reply":"2021-11-14T15:00:25.901752Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"test_features[:5]","metadata":{"papermill":{"duration":0.099334,"end_time":"2021-10-23T12:32:41.13773","exception":false,"start_time":"2021-10-23T12:32:41.038396","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T15:00:27.192037Z","iopub.execute_input":"2021-11-14T15:00:27.192594Z","iopub.status.idle":"2021-11-14T15:00:27.201212Z","shell.execute_reply.started":"2021-11-14T15:00:27.192555Z","shell.execute_reply":"2021-11-14T15:00:27.200512Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"print(train_features.shape)\nprint(test_features.shape)","metadata":{"papermill":{"duration":0.094271,"end_time":"2021-10-23T12:32:41.318218","exception":false,"start_time":"2021-10-23T12:32:41.223947","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T15:00:28.432320Z","iopub.execute_input":"2021-11-14T15:00:28.432998Z","iopub.status.idle":"2021-11-14T15:00:28.439474Z","shell.execute_reply.started":"2021-11-14T15:00:28.432960Z","shell.execute_reply":"2021-11-14T15:00:28.438668Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"### 4.2. Create Data Splits","metadata":{"papermill":{"duration":0.086211,"end_time":"2021-10-23T12:32:41.491361","exception":false,"start_time":"2021-10-23T12:32:41.40515","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_x, val_x, train_y, val_y = train_test_split(\n    train_features,\n    target,\n    test_size=0.2,\n    random_state=RANDOM_SEED,\n)\n\nprint(train_x.shape)\nprint(train_y.shape)\nprint(val_x.shape)\nprint(val_y.shape)","metadata":{"papermill":{"duration":0.101088,"end_time":"2021-10-23T12:32:41.678491","exception":false,"start_time":"2021-10-23T12:32:41.577403","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T15:00:32.357433Z","iopub.execute_input":"2021-11-14T15:00:32.357995Z","iopub.status.idle":"2021-11-14T15:00:32.370788Z","shell.execute_reply.started":"2021-11-14T15:00:32.357956Z","shell.execute_reply":"2021-11-14T15:00:32.369921Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"### 4.3. Create TensorFlow Datasets","metadata":{"papermill":{"duration":0.086268,"end_time":"2021-10-23T12:32:41.851746","exception":false,"start_time":"2021-10-23T12:32:41.765478","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Create TensorFlow Datasets \ntrain_ds = tf.data.Dataset.from_tensor_slices((dict(train_x), train_y))\nval_ds = tf.data.Dataset.from_tensor_slices((dict(val_x), val_y))\ntest_ds = tf.data.Dataset.from_tensor_slices(dict(test_features))","metadata":{"papermill":{"duration":0.1074,"end_time":"2021-10-23T12:32:42.047027","exception":false,"start_time":"2021-10-23T12:32:41.939627","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T15:00:36.067012Z","iopub.execute_input":"2021-11-14T15:00:36.067569Z","iopub.status.idle":"2021-11-14T15:00:36.086369Z","shell.execute_reply.started":"2021-11-14T15:00:36.067532Z","shell.execute_reply":"2021-11-14T15:00:36.085579Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"AUTOTUNE = tf.data.experimental.AUTOTUNE\n\nBUFFER_SIZE = 1000\nBATCH_SIZE = 32\n\ndef configure_dataset(dataset, shuffle=False, test=False):\n    # Configure the tf dataset for cache, shuffle, batch, and prefetch\n    if shuffle:\n        dataset = dataset.cache()\\\n                        .shuffle(BUFFER_SIZE, seed=RANDOM_SEED, reshuffle_each_iteration=True)\\\n                        .batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)\n    elif test:\n        dataset = dataset.cache()\\\n                        .batch(BATCH_SIZE, drop_remainder=False).prefetch(AUTOTUNE)\n    else:\n        dataset = dataset.cache()\\\n                        .batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)\n    return dataset","metadata":{"papermill":{"duration":0.095147,"end_time":"2021-10-23T12:32:42.228344","exception":false,"start_time":"2021-10-23T12:32:42.133197","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T15:00:52.132637Z","iopub.execute_input":"2021-11-14T15:00:52.133349Z","iopub.status.idle":"2021-11-14T15:00:52.139496Z","shell.execute_reply.started":"2021-11-14T15:00:52.133311Z","shell.execute_reply":"2021-11-14T15:00:52.138714Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# Configure the datasets\ntrain_ds = configure_dataset(train_ds, shuffle=True)\nval_ds = configure_dataset(val_ds)\ntest_ds = configure_dataset(test_ds, test=True)","metadata":{"papermill":{"duration":0.107724,"end_time":"2021-10-23T12:32:42.422402","exception":false,"start_time":"2021-10-23T12:32:42.314678","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T15:00:54.642315Z","iopub.execute_input":"2021-11-14T15:00:54.643133Z","iopub.status.idle":"2021-11-14T15:00:54.653736Z","shell.execute_reply.started":"2021-11-14T15:00:54.643061Z","shell.execute_reply":"2021-11-14T15:00:54.652827Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# Print the dataset specifications\nprint(train_ds.element_spec)\nprint(val_ds.element_spec)\nprint(test_ds.element_spec)","metadata":{"papermill":{"duration":0.099372,"end_time":"2021-10-23T12:32:42.60802","exception":false,"start_time":"2021-10-23T12:32:42.508648","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T15:00:55.842065Z","iopub.execute_input":"2021-11-14T15:00:55.842966Z","iopub.status.idle":"2021-11-14T15:00:55.849439Z","shell.execute_reply.started":"2021-11-14T15:00:55.842914Z","shell.execute_reply":"2021-11-14T15:00:55.848610Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"# 5. Building the Classifier Model\n\nFor this task we will be using a pre-trained BERT model loaded from TensorFlow Hub. This model has 12 hidden layers, a hidden unit size of 768, and 12 attention heads. It has a companion preprocessor that is loaded from the same repository. This preprocessor takes text segments and converts them to numeric token ids accepted by the BERT encoder. These token ids are:\n* input_word_ids\n    - ids of the input sequences\n* input_mask\n    - represents all pre-padded input tokens as 1, and padded tokens as 0\n* input_type_ids\n    - contains indices for each input segment with padding locations indexed at 0","metadata":{"papermill":{"duration":0.086528,"end_time":"2021-10-23T12:32:42.785723","exception":false,"start_time":"2021-10-23T12:32:42.699195","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### 5.1. Load the Pre-trained BERT Encoder ","metadata":{"papermill":{"duration":0.087986,"end_time":"2021-10-23T12:32:42.961039","exception":false,"start_time":"2021-10-23T12:32:42.873053","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# BERT encoder w/ preprocessor\nbert_preprocessor = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3', name='BERT_preprocesser')\nbert_encoder = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4', trainable=True, name='BERT_encoder')\n# Keyword embedding layer\nnnlm_embed = hub.KerasLayer('https://tfhub.dev/google/nnlm-en-dim50/2', name='embedding_layer')","metadata":{"papermill":{"duration":24.516365,"end_time":"2021-10-23T12:33:07.565057","exception":false,"start_time":"2021-10-23T12:32:43.048692","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T15:01:03.272905Z","iopub.execute_input":"2021-11-14T15:01:03.273179Z","iopub.status.idle":"2021-11-14T15:01:25.569254Z","shell.execute_reply.started":"2021-11-14T15:01:03.273148Z","shell.execute_reply":"2021-11-14T15:01:25.568502Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"### 5.2. Model Composition\n\nTo build our classifier we will be using the TensorFlow functional API, this will reduce constraints on our model design. Two input branches will be merged into a classification layer. The first branch is a text input layer that feeds into the BERT preprocessor. This layer is passed to the BERT encoder and is returned as a pooled output. This output is then regularized with a dropout layer. \n\nOn the second branch, a keyword input is passed to a pre-trained word embedding layer. The embeddings are flattened and passed into a dense neural net, then fed into a dropout layer. \n\nThe layer outputs from each model are concatenated, passed into a dense neural net with dropout, then sent to a single unit dense classification layer with a sigmoid activation. The sigmoid activation function will return class probabilities that we can use to plot a Receiver Operating Characteristic ([ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)) curve and a [Confusion Matrix](https://en.wikipedia.org/wiki/Confusion_matrix) to analyze our results. ","metadata":{"papermill":{"duration":0.08899,"end_time":"2021-10-23T12:33:07.7494","exception":false,"start_time":"2021-10-23T12:33:07.66041","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def build_model():\n    \n    # Construct text layers\n    text_input = layers.Input(shape=(), dtype=tf.string, name='clean_text') # Name matches df heading\n    encoder_inputs = bert_preprocessor(text_input)\n    encoder_outputs = bert_encoder(encoder_inputs)\n    # pooled_output returns [batch_size, hidden_layers]\n    pooled_output = encoder_outputs[\"pooled_output\"]          \n    bert_dropout = layers.Dropout(0.1, name='BERT_dropout')(pooled_output)   \n    \n    # Construct keyword layers\n    key_input = layers.Input(shape=(), dtype=tf.string, name='keyword_fill') # Name matches df heading\n    key_embed = nnlm_embed(key_input)\n    key_flat = layers.Flatten()(key_embed)\n    key_dense = layers.Dense(128, activation='elu', kernel_regularizer=regularizers.l2(1e-4))(key_flat)\n    key_dropout = layers.Dropout(0.5, name='dense_dropout')(key_dense)\n    \n    # Merge the layers and classify \n    merge = layers.concatenate([bert_dropout, key_dropout])\n    dense = layers.Dense(128, activation='elu', kernel_regularizer=regularizers.l2(1e-4))(merge)\n    dropout = layers.Dropout(0.5, name='merged_dropout')(dense)    \n    clf = layers.Dense(1, activation='sigmoid', name='classifier')(dropout)\n\n    return Model([text_input, key_input], clf, name='BERT_classifier')","metadata":{"papermill":{"duration":0.10073,"end_time":"2021-10-23T12:33:07.93693","exception":false,"start_time":"2021-10-23T12:33:07.8362","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T15:01:28.082111Z","iopub.execute_input":"2021-11-14T15:01:28.082763Z","iopub.status.idle":"2021-11-14T15:01:28.091590Z","shell.execute_reply.started":"2021-11-14T15:01:28.082726Z","shell.execute_reply":"2021-11-14T15:01:28.090956Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"bert_classifier = build_model()\nbert_classifier.summary()","metadata":{"papermill":{"duration":0.831378,"end_time":"2021-10-23T12:33:08.855511","exception":false,"start_time":"2021-10-23T12:33:08.024133","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T15:01:30.712279Z","iopub.execute_input":"2021-11-14T15:01:30.714281Z","iopub.status.idle":"2021-11-14T15:01:31.886033Z","shell.execute_reply.started":"2021-11-14T15:01:30.714225Z","shell.execute_reply":"2021-11-14T15:01:31.885330Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(bert_classifier, show_shapes=False, dpi=96)","metadata":{"papermill":{"duration":0.575985,"end_time":"2021-10-23T12:33:09.521281","exception":false,"start_time":"2021-10-23T12:33:08.945296","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T15:01:35.197388Z","iopub.execute_input":"2021-11-14T15:01:35.197677Z","iopub.status.idle":"2021-11-14T15:01:37.796558Z","shell.execute_reply.started":"2021-11-14T15:01:35.197647Z","shell.execute_reply":"2021-11-14T15:01:37.795752Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"For training the BERT model we'll use an Adam optimizer with weight decay ([AdamW](https://arxiv.org/abs/1711.05101)). This method differs from the standard Adam algorithm with its use of decoupled weight decay regularization. This is the optimizer that BERT was originally trained with. It has a linear warm-up period over the first 10% of training steps paired with a lower learning rate. One of the simpler ways to implement this [optimizer](https://github.com/tensorflow/models/blob/master/official/nlp/optimization.py) is with the TensorFlow official models collection.","metadata":{"papermill":{"duration":0.0878,"end_time":"2021-10-23T12:33:09.69797","exception":false,"start_time":"2021-10-23T12:33:09.61017","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### 5.3. Construct the AdamW Optimizer","metadata":{"papermill":{"duration":0.088118,"end_time":"2021-10-23T12:33:09.874013","exception":false,"start_time":"2021-10-23T12:33:09.785895","status":"completed"},"tags":[]}},{"cell_type":"code","source":"EPOCHS = 5\nLEARNING_RATE = 5e-5\n\nSTEPS_PER_EPOCH = int(train_ds.unbatch().cardinality().numpy() / BATCH_SIZE)\nVAL_STEPS = int(val_ds.unbatch().cardinality().numpy() / BATCH_SIZE)\n# Calculate the train and warmup steps for the optimizer\nTRAIN_STEPS = STEPS_PER_EPOCH * EPOCHS\nWARMUP_STEPS = int(TRAIN_STEPS * 0.1)\n\nadamw_optimizer = create_optimizer(\n    init_lr=LEARNING_RATE,\n    num_train_steps=TRAIN_STEPS,\n    num_warmup_steps=WARMUP_STEPS\n)","metadata":{"papermill":{"duration":0.112748,"end_time":"2021-10-23T12:33:10.075207","exception":false,"start_time":"2021-10-23T12:33:09.962459","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T15:01:41.502593Z","iopub.execute_input":"2021-11-14T15:01:41.503372Z","iopub.status.idle":"2021-11-14T15:01:41.529108Z","shell.execute_reply.started":"2021-11-14T15:01:41.503332Z","shell.execute_reply":"2021-11-14T15:01:41.528428Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"# 6. Train the Classifier","metadata":{"papermill":{"duration":0.08739,"end_time":"2021-10-23T12:33:10.249988","exception":false,"start_time":"2021-10-23T12:33:10.162598","status":"completed"},"tags":[]}},{"cell_type":"code","source":"bert_classifier.compile(\n    loss=BinaryCrossentropy(from_logits=True), \n    optimizer= adamw_optimizer,\n    metrics=[BinaryAccuracy(name='accuracy')]\n)\n\nhistory = bert_classifier.fit(\n    train_ds, \n    epochs=EPOCHS,\n    steps_per_epoch=STEPS_PER_EPOCH,    \n    validation_data= val_ds,\n    validation_steps=VAL_STEPS\n)","metadata":{"papermill":{"duration":236.542108,"end_time":"2021-10-23T12:37:06.891097","exception":false,"start_time":"2021-10-23T12:33:10.348989","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T15:01:45.712716Z","iopub.execute_input":"2021-11-14T15:01:45.712993Z","iopub.status.idle":"2021-11-14T15:11:12.957322Z","shell.execute_reply.started":"2021-11-14T15:01:45.712962Z","shell.execute_reply":"2021-11-14T15:11:12.956607Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"# 7. Visualize Results   ","metadata":{"papermill":{"duration":0.184136,"end_time":"2021-10-23T12:37:07.262161","exception":false,"start_time":"2021-10-23T12:37:07.078025","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### 7.1. Plot the Loss and Accuracy Metrics","metadata":{"papermill":{"duration":0.182765,"end_time":"2021-10-23T12:37:07.630594","exception":false,"start_time":"2021-10-23T12:37:07.447829","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Assign the loss and accuracy metrics\ntrain_loss = history.history['loss']\nval_loss = history.history['val_loss']\ntrain_acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']","metadata":{"papermill":{"duration":0.191166,"end_time":"2021-10-23T12:37:08.005126","exception":false,"start_time":"2021-10-23T12:37:07.81396","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T15:11:16.516466Z","iopub.execute_input":"2021-11-14T15:11:16.516731Z","iopub.status.idle":"2021-11-14T15:11:16.521356Z","shell.execute_reply.started":"2021-11-14T15:11:16.516700Z","shell.execute_reply":"2021-11-14T15:11:16.520433Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"# Plot the training and validation metrics\nfig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nsns.lineplot(ax=ax1, data = train_acc, label=f'Training Accuracy')\nsns.lineplot(ax=ax1, data = val_acc, label=f'Validation Accuracy')\nsns.lineplot(ax=ax2, data = train_loss, label=f'Training Loss')\nsns.lineplot(ax=ax2, data = val_loss, label=f'Validation Loss')\nax1.set_ylabel('Accuracy')\nax1.set_xlim(xmin=0)\nax2.set_ylabel('Loss')\nax2.set_xlabel('Epochs')\nax2.set_xlim(xmin=0)\nplt.suptitle('History')\nplt.show()","metadata":{"papermill":{"duration":0.773409,"end_time":"2021-10-23T12:37:08.96224","exception":false,"start_time":"2021-10-23T12:37:08.188831","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T15:11:18.866518Z","iopub.execute_input":"2021-11-14T15:11:18.866802Z","iopub.status.idle":"2021-11-14T15:11:19.377576Z","shell.execute_reply.started":"2021-11-14T15:11:18.866770Z","shell.execute_reply":"2021-11-14T15:11:19.376898Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"### 7.2. Plot the ROC Curve","metadata":{"papermill":{"duration":0.185214,"end_time":"2021-10-23T12:37:09.348492","exception":false,"start_time":"2021-10-23T12:37:09.163278","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Get the array of labels from the validation Dataset\nval_target = np.asarray([i[1] for i in list(val_ds.unbatch().as_numpy_iterator())])\nprint(val_target.shape)\nval_target[:5]","metadata":{"papermill":{"duration":0.421802,"end_time":"2021-10-23T12:37:09.957115","exception":false,"start_time":"2021-10-23T12:37:09.535313","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T15:11:24.391654Z","iopub.execute_input":"2021-11-14T15:11:24.391923Z","iopub.status.idle":"2021-11-14T15:11:24.638563Z","shell.execute_reply.started":"2021-11-14T15:11:24.391894Z","shell.execute_reply":"2021-11-14T15:11:24.637432Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"# Get predictions from the validation Dataset\nval_predict = bert_classifier.predict(val_ds)","metadata":{"papermill":{"duration":10.369488,"end_time":"2021-10-23T12:37:20.513182","exception":false,"start_time":"2021-10-23T12:37:10.143694","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T15:11:26.391530Z","iopub.execute_input":"2021-11-14T15:11:26.392127Z","iopub.status.idle":"2021-11-14T15:11:36.701974Z","shell.execute_reply.started":"2021-11-14T15:11:26.392071Z","shell.execute_reply":"2021-11-14T15:11:36.701222Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"# Get the false positive and true positive rates\nfpr, tpr, _ = roc_curve(val_target, val_predict)\nplt.plot(fpr, tpr, color='orange')\nplt.plot([0,1], [0,1], linestyle='--')\nplt.title('Validation ROC Curve')\nplt.xlabel('False Positives (%)')\nplt.ylabel('True Positives (%)')\nplt.grid(True)    \nplt.show()","metadata":{"papermill":{"duration":0.375277,"end_time":"2021-10-23T12:37:21.074619","exception":false,"start_time":"2021-10-23T12:37:20.699342","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T15:11:41.166774Z","iopub.execute_input":"2021-11-14T15:11:41.167354Z","iopub.status.idle":"2021-11-14T15:11:41.414590Z","shell.execute_reply.started":"2021-11-14T15:11:41.167315Z","shell.execute_reply":"2021-11-14T15:11:41.413906Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"### 7.3. Plot the Confusion Matrix","metadata":{"papermill":{"duration":0.186593,"end_time":"2021-10-23T12:37:21.449078","exception":false,"start_time":"2021-10-23T12:37:21.262485","status":"completed"},"tags":[]}},{"cell_type":"code","source":"THRESHOLD = 0.5 # Default value\n# Get the true negative, false positive, false negative, and true positive values\ntn, fp, fn, tp = confusion_matrix(val_target, val_predict > THRESHOLD).flatten()\n# Construct the dataframe\ncm = pd.DataFrame(\n                    [[tn, fp], [fn, tp]], \n                    index=['No Disaster', 'Disaster'], \n                    columns=['No Disaster', 'Disaster']\n)\n# Plot the matrix\nsns.heatmap(cm, annot=True, fmt='g')    \nplt.title('Validation Confusion Matrix')\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()","metadata":{"papermill":{"duration":0.443717,"end_time":"2021-10-23T12:37:22.079777","exception":false,"start_time":"2021-10-23T12:37:21.63606","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T15:11:46.046822Z","iopub.execute_input":"2021-11-14T15:11:46.047397Z","iopub.status.idle":"2021-11-14T15:11:46.322724Z","shell.execute_reply.started":"2021-11-14T15:11:46.047357Z","shell.execute_reply":"2021-11-14T15:11:46.321947Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"# 8. Predictions on the Test Data","metadata":{"papermill":{"duration":0.19714,"end_time":"2021-10-23T12:37:22.465594","exception":false,"start_time":"2021-10-23T12:37:22.268454","status":"completed"},"tags":[]}},{"cell_type":"code","source":"predictions = bert_classifier.predict(test_ds)\nprint(predictions.shape)\nprint(predictions[:5])","metadata":{"papermill":{"duration":21.818374,"end_time":"2021-10-23T12:37:44.471442","exception":false,"start_time":"2021-10-23T12:37:22.653068","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T15:11:59.891788Z","iopub.execute_input":"2021-11-14T15:11:59.892523Z","iopub.status.idle":"2021-11-14T15:12:21.916449Z","shell.execute_reply.started":"2021-11-14T15:11:59.892469Z","shell.execute_reply":"2021-11-14T15:12:21.914737Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"### 8.1. Predictions Histogram","metadata":{"papermill":{"duration":0.187719,"end_time":"2021-10-23T12:37:44.84976","exception":false,"start_time":"2021-10-23T12:37:44.662041","status":"completed"},"tags":[]}},{"cell_type":"code","source":"count, bin_edges = np.histogram(predictions)\nsns.histplot(predictions, bins=bin_edges, legend=False)\nplt.axvline(x=THRESHOLD, linestyle='--', color='black', label='Threshold')\nplt.title('Predicted Probability of Disaster')\nplt.xlabel('Probabilities')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()","metadata":{"papermill":{"duration":0.48692,"end_time":"2021-10-23T12:37:45.52792","exception":false,"start_time":"2021-10-23T12:37:45.041","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T15:12:21.918205Z","iopub.execute_input":"2021-11-14T15:12:21.918588Z","iopub.status.idle":"2021-11-14T15:12:22.237594Z","shell.execute_reply.started":"2021-11-14T15:12:21.918543Z","shell.execute_reply":"2021-11-14T15:12:22.236915Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"This distribution represents the binary nature of the prediction values. The threshold marks the line by which predictions will be considered either non-disasters (below threshold) or disasters (above threshold). The default value for a binary accuracy threshold is `0.5`. Finally we use this threshold to label and submit the predictions.","metadata":{"papermill":{"duration":0.189694,"end_time":"2021-10-23T12:37:45.908704","exception":false,"start_time":"2021-10-23T12:37:45.71901","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### 8.2. Label the Predictions","metadata":{"papermill":{"duration":0.191661,"end_time":"2021-10-23T12:37:46.290662","exception":false,"start_time":"2021-10-23T12:37:46.099001","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Use the threshold to label predictions\npredictions = np.where(predictions > THRESHOLD, 1, 0)\ndf_predictions = pd.DataFrame(predictions)\ndf_predictions.columns = ['target']\n\nprint(df_predictions.shape)\ndf_predictions.head()","metadata":{"papermill":{"duration":0.204798,"end_time":"2021-10-23T12:37:46.689367","exception":false,"start_time":"2021-10-23T12:37:46.484569","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T15:12:29.891581Z","iopub.execute_input":"2021-11-14T15:12:29.891855Z","iopub.status.idle":"2021-11-14T15:12:29.902860Z","shell.execute_reply.started":"2021-11-14T15:12:29.891824Z","shell.execute_reply":"2021-11-14T15:12:29.902143Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"### 8.3. Submit the Predictions","metadata":{"papermill":{"duration":0.189498,"end_time":"2021-10-23T12:37:47.070557","exception":false,"start_time":"2021-10-23T12:37:46.881059","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Concatenate the columns and convert submission to csv\nsubmission = pd.concat([df_test['id'], df_predictions], axis=1)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"papermill":{"duration":0.209913,"end_time":"2021-10-23T12:37:47.470916","exception":false,"start_time":"2021-10-23T12:37:47.261003","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-14T15:12:36.894309Z","iopub.execute_input":"2021-11-14T15:12:36.894961Z","iopub.status.idle":"2021-11-14T15:12:36.911175Z","shell.execute_reply.started":"2021-11-14T15:12:36.894922Z","shell.execute_reply":"2021-11-14T15:12:36.910356Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.194747,"end_time":"2021-10-23T12:37:47.854992","exception":false,"start_time":"2021-10-23T12:37:47.660245","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}